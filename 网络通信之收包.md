[网络通信之收包](https://ty-chen.github.io/linux-kernel-tcp-receive/)

### 网卡驱动层：通过 NAPI（New API）或中断机制向内核提交数据包。
NAPI:
网络吞吐量比较大的时候，网络包的到达会十分频繁。这个时候，如果非常频繁地去触发中断，会造成频繁的上下文切换，带来极大的开销。因此硬件处理厂商设计了一种机制，就是当一些网络包到来触发了中断，内核处理完这些网络包之后，我们可以先进入主动轮询 poll 网卡的方式主动去接收到来的网络包。如果一直有，就一直处理，等处理告一段落，就返回干其他的事情。当再有下一批网络包到来的时候，再中断，再轮询 poll。这样就会大大减少中断的数量，提升网络处理的效率，这种处理方式我们称为 NAPI。

网卡被激活的时候调用ice_open(){
	注册硬件的中断处理函数（中断处理函数调用__napi_schedule(){}）
}，
__napi_schedule() 处于中断处理的关键部分，在被调用的时候，中断是暂时关闭的。处理网络包是个复杂的过程，需要到中断处理的延迟处理部分执行，所以 ____napi_schedule() 将当前设备放到 struct softnet_data 结构的 poll_list 里面，说明在延迟处理部分可以接着处理这个 poll_list 里面的网络设备。然后 ____napi_schedule() 触发一个软中断 NET_RX_SOFTIRQ，通过软中断触发中断处理的延迟处理部分，也是常用的手段。

中断 NET_RX_SOFTIRQ 对应的中断处理函数是 net_rx_action()

sd->poll_list用于网络包接收
从 poll_list 里面取出有网络包到达的设备，然后调用 napi_poll() 来轮询这些设备，napi_poll() 会调用最初设备初始化的时候注册的 poll 函数，对于 ixgb_driver对应的函数是 ixgb_clean() //在probe的时候注册的

ixgb_check_copybreak() 函数将 buffer_info 里面的内容拷贝到 struct sk_buff *skb，从而可以作为一个网络包进行后续的处理，然后调用 netif_receive_skb()进入MAC层继续进行收包的解析处理。

### MAC层
从 netif_receive_skb() 函数开始，我们就进入了内核的网络协议栈。接下来的调用链为：netif_receive_skb()->netif_receive_skb_internal()->__netif_receive_skb()->__netif_receive_skb_core()。在 __netif_receive_skb_core() 中，我们先是处理了二层的一些逻辑，如对于 VLAN 的处理，如果不是则调用deliver_ptype_list_skb() 在一个协议列表中逐个匹配在网络包 struct sk_buff 里面定义的 skb->protocol，该变量表示三层使用的协议类型。

无论是VLAN还是普通的包，最后的发送均会调用deliver_skb()，该函数会调用协议定义好的函数进行网络层解析。对于IP协议即为ip_rcv()。

*网卡硬件支持MAC地址过滤，检查MAC地址是否为本机或广播地址以及进行CRC校验*

### 网络层
ip_rcv(){
	ip_rcv_finish(){
	//ip_rcv_finish() 首先调用ip_rcv_finish_core()，该函数会先检测是否为广播、组播，如果不是则得到网络包对应的路由表，然后调用 dst_input()，在 dst_input() 中，调用的是 struct rtable 的成员的 dst 的 input() 函数。在 rt_dst_alloc() 中，我们可以看到input 函数指向的是 ip_local_deliver()。
	}
}
进入ip_local_deliver()意味着从PREROUTING确认进入本机处理，所以进入了状态INPUT，如果 IP 层进行了分段，则进行重新的组合。接下来就是我们熟悉的 NF_HOOK。在经过 iptables 规则处理完毕后，会调用 ip_local_deliver_finish()。
ip_local_deliver_finish()首先调用__skb_pull()从sk_buff中取下一个，接着调用ip_protocol_deliver_rcu()，该函数会从inet_protos[protocol]中找寻对应的处理函数进一步对收到的数据包进行解析。对应TCP的是tcp_v4_rcv()，UDP则是udp_rcv()。
 - ​TCP：处理序列号、确认机制、流量控制、拥塞控制，完成连接建立（三次握手）、数据分段重组。
​ - UDP：直接无连接传输，仅校验端口号和长度。

*NAT转换也是在网络层完成的*

### 五. 传输层
tcp_v4_rcv()

将数据包转发到对应进程的监听端口

通过 epoll、kqueue 或 select 等机制管理多个套接字（Socket）。

​查找连接（四元组匹配）​：
- 根据目的 IP (dst_ip)、目的端口 (dst_port)、源 IP (src_ip)、源端口 (src_port) 组成的 ​四元组，在 tcp_hashinfo 的哈希表中查找对应的 struct sock（套接字）。
- 如果存在已建立的连接（如 ESTABLISHED 状态），则直接将数据包加入该连接的接收队列。
- 如果是 SYN 包（建立新连接），则触发 tcp_v4_connect() 处理。
数据包队列入队：
- 找到的 struct sock 的 sk_receive_queue 队列中追加 skb。
- 唤醒等待读取数据的进程（通过 sk_wake_up 触发软中断）。

####套接字的注册与查找
​**(1) 套接字注册**
当进程调用 bind() 绑定端口时：
- ​TCP：在 tcp_hashinfo 中为 (port, net)（网络命名空间）注册 struct sock。
- ​UDP：在 udp_hashinfo 中为 (port, net) 注册 struct sock。
- ​哈希冲突解决：使用链表或红黑树存储同一端口/网络的多个套接字。

#### 应用层
- 通过套接字（Socket）将数据传递给目标进程。
- 常见服务绑定方式：inetd、systemd 监听端口，或进程主动调用 bind()/listen()。

### 六. 套接字层
当接收的网络包进入各种队列之后，接下来我们就要等待用户进程去读取它们了。读取一个 socket，就像读取一个文件一样，读取 socket 的文件描述符，通过 read 系统调用。read 系统调用对于一个文件描述符的操作，大致过程都是类似的，在文件系统那一节，我们已经详细解析过。最终它会调用到用来表示一个打开文件的结构 stuct file 指向的 file_operations 操作。
sock_recvmsg()实际调用sock_recvmmsg_nosec()，该函数会调用套接字对应的读操作，即inet_recvmsg()。
inet_recvmsg()会调用协议对应的读操作，即tcp_recvmsg()进行读操作。

## 关键优化技术
- ​零拷贝（Zero-Copy）​：直接在内核空间处理数据，减少 CPU 和内存占用（如 splice()、vmsplice()）。
- ​RSS（Receive Side Scaling）​：多核环境下将数据包分发给不同 CPU 核处理。
- ​XDP/eBPF：在网络层实现高效的数据包过滤或处理（无需进入内核协议栈）。

